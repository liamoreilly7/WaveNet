{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchaudio\n","import sys\n","\n","import matplotlib.pyplot as plt\n","import IPython.display as ipd\n","\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{},"source":["### Importing the Data"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from torchaudio.datasets import SPEECHCOMMANDS\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import os\n","\n","class SubsetSC(SPEECHCOMMANDS):\n","    def __init__(self, subset: str = None):\n","        super().__init__(\"./\", download=True)\n","\n","        def load_list(filename):\n","            filepath = os.path.join(self._path, filename)\n","            with open(filepath) as fileobj:\n","                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n","\n","        if subset == \"validation\":\n","            self._walker = load_list(\"validation_list.txt\")\n","        elif subset == \"testing\":\n","            self._walker = load_list(\"testing_list.txt\")\n","        elif subset == \"training\":\n","            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n","            excludes = set(excludes)\n","            self._walker = [w for w in self._walker if w not in excludes]"]},{"cell_type":"markdown","metadata":{},"source":["Load the data locally and create training and testing split of the data."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["train_set = SubsetSC(\"training\")\n","test_set = SubsetSC(\"testing\")\n","\n","waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]"]},{"cell_type":"markdown","metadata":{},"source":["A data point in the SPEECHCOMMANDS dataset is a tuple made of a waveform (the audio signal), the sample rate, the utterance (label), the ID of the speaker, the number of the utterance."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Shape of waveform: {}\".format(waveform.size()))\n","print(\"Sample rate of waveform: {}\".format(sample_rate))\n","\n","plt.plot(waveform.t().numpy())"]},{"cell_type":"markdown","metadata":{},"source":["### Formatting the Data"]},{"cell_type":"markdown","metadata":{},"source":["Downsample the audio for faster processing with the hope of not losing too much of the classification power."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_sample_rate = 8000\n","transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)\n","transformed = transform(waveform)\n","\n","ipd.Audio(transformed.numpy(), rate=new_sample_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def pad_sequence(batch):\n","    # Make all tensor in a batch the same length by padding with zeros\n","    batch = [item.t() for item in batch]\n","    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n","    return batch.permute(0, 2, 1)"]},{"cell_type":"markdown","metadata":{},"source":["To turn a list of data point made of audio recordings and utterances into two batched tensors for the model, we implement a collate function which is used by the PyTorch DataLoader that allows us to iterate over a dataset by batches. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labels = sorted(list(set(datapoint[2] for datapoint in train_set)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def label_to_index(word):\n","    # Return the position of the word in labels\n","    return torch.tensor(labels.index(word))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def collate_fn(batch):\n","\n","    # A data tuple has the form:\n","    # waveform, sample_rate, label, speaker_id, utterance_number\n","\n","    tensors, targets = [], []\n","\n","    # Gather in lists, and encode labels as indices\n","    for waveform, _, label, *_ in batch:\n","        tensors += [waveform]\n","        targets += [label_to_index(label)]\n","\n","    # Group the list of tensors into a batched tensor\n","    tensors = pad_sequence(tensors)\n","    targets = torch.stack(targets)\n","\n","    return tensors, targets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch_size = 256\n","num_workers = 0\n","pin_memory = False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(\n","    train_set,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    num_workers=num_workers,\n","    pin_memory=pin_memory,\n",")\n","test_loader = torch.utils.data.DataLoader(\n","    test_set,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    drop_last=False,\n","    collate_fn=collate_fn,\n","    num_workers=num_workers,\n","    pin_memory=pin_memory,\n",")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":2}
