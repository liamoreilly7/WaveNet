{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchaudio\n","import sys\n","import numpy as np\n","import torch.utils.data as data\n","import librosa\n","\n","import matplotlib.pyplot as plt\n","import IPython.display as ipd\n","\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{},"source":["### Importing the Data"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from torchaudio.datasets import SPEECHCOMMANDS\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import os\n","\n","class SubsetSC(SPEECHCOMMANDS):\n","    def __init__(self, subset: str = None):\n","        super().__init__(\"./\", download=True)\n","\n","        def load_list(filename):\n","            filepath = os.path.join(self._path, filename)\n","            with open(filepath) as fileobj:\n","                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n","\n","        if subset == \"validation\":\n","            self._walker = load_list(\"validation_list.txt\")\n","        elif subset == \"testing\":\n","            self._walker = load_list(\"testing_list.txt\")\n","        elif subset == \"training\":\n","            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n","            excludes = set(excludes)\n","            self._walker = [w for w in self._walker if w not in excludes]"]},{"cell_type":"markdown","metadata":{},"source":["Load the data locally and create training and testing split of the data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_set = SubsetSC(\"training\")\n","test_set = SubsetSC(\"testing\")\n","\n","waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]"]},{"cell_type":"markdown","metadata":{},"source":["A data point in the SPEECHCOMMANDS dataset is a tuple made of a waveform (the audio signal), the sample rate, the utterance (label), the ID of the speaker, the number of the utterance."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Shape of waveform: {}\".format(waveform.size()))\n","print(\"Sample rate of waveform: {}\".format(sample_rate))\n","\n","plt.plot(waveform.t().numpy())"]},{"cell_type":"markdown","metadata":{},"source":["### Formatting the Data"]},{"cell_type":"markdown","metadata":{},"source":["Downsample the audio for faster processing with the hope of not losing too much of the classification power."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_sample_rate = 8000\n","transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)\n","transformed = transform(waveform)\n","\n","ipd.Audio(transformed.numpy(), rate=new_sample_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def pad_sequence(batch):\n","    # Make all tensor in a batch the same length by padding with zeros\n","    batch = [item.t() for item in batch]\n","    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n","    return batch.permute(0, 2, 1)"]},{"cell_type":"markdown","metadata":{},"source":["To turn a list of data point made of audio recordings and utterances into two batched tensors for the model, we implement a collate function which is used by the PyTorch DataLoader that allows us to iterate over a dataset by batches. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labels = sorted(list(set(datapoint[2] for datapoint in train_set)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def label_to_index(word):\n","    # Return the position of the word in labels\n","    return torch.tensor(labels.index(word))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def collate_fn(batch):\n","\n","    # A data tuple has the form:\n","    # waveform, sample_rate, label, speaker_id, utterance_number\n","\n","    tensors, targets = [], []\n","\n","    # Gather in lists, and encode labels as indices\n","    for waveform, _, label, *_ in batch:\n","        tensors += [waveform]\n","        targets += [label_to_index(label)]\n","\n","    # Group the list of tensors into a batched tensor\n","    tensors = pad_sequence(tensors)\n","    targets = torch.stack(targets)\n","\n","    return tensors, targets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch_size = 256\n","num_workers = 0\n","pin_memory = False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(\n","    train_set,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    num_workers=num_workers,\n","    pin_memory=pin_memory,\n",")\n","test_loader = torch.utils.data.DataLoader(\n","    test_set,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    drop_last=False,\n","    collate_fn=collate_fn,\n","    num_workers=num_workers,\n","    pin_memory=pin_memory,\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def load_audio(filename, sample_rate=16000, trim=True, trim_frame_length=2048):\n","    audio, _ = librosa.load(filename, sr=sample_rate, mono=True)\n","    audio = audio.reshape(-1, 1)\n","\n","    if trim > 0:\n","        audio, _ = librosa.effects.trim(audio, frame_length=trim_frame_length)\n","\n","    return audio\n","\n","\n","def one_hot_encode(data, channels=256):\n","    one_hot = np.zeros((data.size, channels), dtype=float)\n","    one_hot[np.arange(data.size), data.ravel()] = 1\n","\n","    return one_hot\n","\n","\n","def one_hot_decode(data, axis=1):\n","    decoded = np.argmax(data, axis=axis)\n","\n","    return decoded\n","\n","\n","def mu_law_encode(audio, quantization_channels=256):\n","    \"\"\"\n","    Quantize waveform amplitudes.\n","    Reference: https://github.com/vincentherrmann/pytorch-wavenet/blob/master/audio_data.py\n","    \"\"\"\n","    mu = float(quantization_channels - 1)\n","    quantize_space = np.linspace(-1, 1, quantization_channels)\n","\n","    quantized = np.sign(audio) * np.log(1 + mu * np.abs(audio)) / np.log(mu + 1)\n","    quantized = np.digitize(quantized, quantize_space) - 1\n","\n","    return quantized\n","\n","\n","def mu_law_decode(output, quantization_channels=256):\n","    \"\"\"\n","    Recovers waveform from quantized values.\n","    Reference: https://github.com/vincentherrmann/pytorch-wavenet/blob/master/audio_data.py\n","    \"\"\"\n","    mu = float(quantization_channels - 1)\n","\n","    expanded = (output / quantization_channels) * 2. - 1\n","    waveform = np.sign(expanded) * (\n","                   np.exp(np.abs(expanded) * np.log(mu + 1)) - 1\n","               ) / mu\n","\n","    return waveform\n","\n","\n","class Dataset(data.Dataset):\n","    def __init__(self, data_dir, sample_rate=16000, in_channels=256, trim=True):\n","        super(Dataset, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.sample_rate = sample_rate\n","        self.trim = trim\n","\n","        self.root_path = data_dir\n","        self.filenames = [x for x in sorted(os.listdir(data_dir))]\n","\n","    def __getitem__(self, index):\n","        filepath = os.path.join(self.root_path, self.filenames[index])\n","\n","        raw_audio = load_audio(filepath, self.sample_rate, self.trim)\n","\n","        encoded_audio = mu_law_encode(raw_audio, self.in_channels)\n","        encoded_audio = one_hot_encode(encoded_audio, self.in_channels)\n","\n","        return encoded_audio\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","\n","class DataLoader(data.DataLoader):\n","    def __init__(self, data_dir, receptive_fields,\n","                 sample_size=0, sample_rate=16000, in_channels=256,\n","                 batch_size=1, shuffle=True):\n","        \"\"\"\n","        DataLoader for WaveNet\n","        :param data_dir:\n","        :param receptive_fields: integer. size(length) of receptive fields\n","        :param sample_size: integer. number of timesteps to train at once.\n","                            sample size has to be bigger than receptive fields.\n","                            |-- receptive field --|---------------------|\n","                            |------- samples -------------------|\n","                            |---------------------|-- outputs --|\n","        :param sample_rate: sound sampling rates\n","        :param in_channels: number of input channels\n","        :param batch_size:\n","        :param shuffle:\n","        \"\"\"\n","        dataset = Dataset(data_dir, sample_rate, in_channels)\n","\n","        super(DataLoader, self).__init__(dataset, batch_size, shuffle)\n","\n","        if sample_size <= receptive_fields:\n","            raise Exception(\"sample_size has to be bigger than receptive_fields\")\n","\n","        self.sample_size = sample_size\n","        self.receptive_fields = receptive_fields\n","\n","        self.collate_fn = self._collate_fn\n","\n","    def calc_sample_size(self, audio):\n","        return self.sample_size if len(audio[0]) >= self.sample_size\\\n","                                else len(audio[0])\n","\n","    @staticmethod\n","    def _variable(data):\n","        tensor = torch.from_numpy(data).float()\n","\n","        if torch.cuda.is_available():\n","            return torch.autograd.Variable(tensor.cuda())\n","        else:\n","            return torch.autograd.Variable(tensor)\n","\n","    def _collate_fn(self, audio):\n","        audio = np.pad(audio, [[0, 0], [self.receptive_fields, 0], [0, 0]], 'constant')\n","\n","        if self.sample_size:\n","            sample_size = self.calc_sample_size(audio)\n","\n","            while sample_size > self.receptive_fields:\n","                inputs = audio[:, :sample_size, :]\n","                targets = audio[:, self.receptive_fields:sample_size, :]\n","\n","                yield self._variable(inputs),\\\n","                      self._variable(one_hot_decode(targets, 2))\n","\n","                audio = audio[:, sample_size-self.receptive_fields:, :]\n","                sample_size = self.calc_sample_size(audio)\n","        else:\n","            targets = audio[:, self.receptive_fields:, :]\n","            return self._variable(audio),\\\n","                   self._variable(one_hot_decode(targets, 2))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["class ResLayer(nn.Module):\n","    def __init__(self, resChannels, skipChannels, dilation):\n","        super(ResLayer, self).__init__()\n","        self.dconv = nn.Conv1d(resChannels, resChannels,\n","                                    kernel_size=2, stride=1,\n","                                    dilation=dilation,\n","                                    padding=0, \n","                                    bias=False)\n","        self.resConv = nn.Conv1d(resChannels, resChannels, 1)\n","        self.skipConv = nn.Conv1d(resChannels, skipChannels, 1)\n","\n","    def forward(self, x, skipSize):\n","        out = self.dconv(x)\n","        tanh = nn.Tanh()\n","        tan = tanh(out)\n","        sigmoid = nn.Sigmoid()\n","        sig = sigmoid(out)\n","        gated = tan * sig\n","        output = self.resConv(gated)\n","        skipOutput = self.skipConv(gated)\n","        skipOutput = skipOutput[:, :, -skipSize:]\n","        return output, skipOutput\n","        "]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["class WaveNetModel(nn.Module):\n","    def __init__(self, layers, layerSize, resChannels, skipChannels):\n","        super(WaveNetModel, self).__init__()\n","        self.resChannels = resChannels\n","        self.skipChannels = skipChannels\n","        dilations = []\n","        #build dilation numbers\n","        [[dilations.append(2**x) for x in range(layerSize)] for i in range(layers)]\n","        #build residual layers\n","        self.resLayers = []\n","        [self.resLayers.append(ResLayer(resChannels, skipChannels, d)) for d in dilations]\n","        #calculate receptive fields\n","        l = []\n","        for i in range(layerSize):\n","            l.append(2**i)\n","        l = l * layers\n","        self.recFields = int(np.sum(l))\n","\n","\n","    def forward(self, x):\n","        output = x.transpose(1, 2)\n","        size = int(output.size(2)) - self.recFields\n","        #Causal convolution\n","        causal = nn.Conv1d(self.skipChannels, self.resChannels, kernel_size=2, stride=1, padding=1, bias=False)\n","        output = causal(output)\n","        output = output[:, :, :-1]\n","        #residual layers\n","        skip = []\n","        for layer in self.resLayers:\n","            output, skipOutput = layer(output, size)\n","            skip.append(skipOutput)\n","        output = torch.stack(skip)\n","        output = torch.sum(output, dim=0)\n","        #skip connections blocks\n","        relu = nn.ReLU()\n","        output = relu(output)\n","        conv1 = nn.Conv1d(self.skipChannels, self.skipChannels, 1)\n","        output = conv1(output)\n","        output = relu(output)\n","        conv2 = nn.Conv1d(self.skipChannels, self.skipChannels, 1)\n","        output = conv2(output)\n","        softmax = nn.Softmax(dim=1)\n","        output = softmax(output)\n","        return output.transpose(1, 2).contiguous()"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def trainModel(model, lr, inputs, targets):\n","    outputs = model.forward(inputs)\n","    loss = nn.CrossEntropyLoss()\n","    model_loss = loss(outputs.view(-1, model.skipChannels), targets.long().view(-1))\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    return loss.data[0]\n","\n","def batch(dataL):\n","    while True:\n","        for dataset in dataL:\n","            for inputs, targets in dataset:\n","                yield inputs, targets\n","\n","@staticmethod\n","def get_model_path(model_dir, step=0):\n","    basename = 'WaveNet'\n","    if step:\n","        return os.path.join(model_dir, '{0}_{1}.pkl'.format(basename, step))\n","    else:\n","        return os.path.join(model_dir, '{0}.pkl'.format(basename))\n","\n","\n","def train(model, lr, numSteps, dataL, saveDir):\n","    steps = 0\n","    for inputs, targets in batch(dataL):\n","        loss = trainModel(model, lr, inputs, targets)\n","        steps += 1\n","        if steps > num_steps:\n","            break\n","    path = get_model_path(saveDir, 0)\n","    torch.save(model.state_dict(), path)\n","    \n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"optimizer got an empty parameter list","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m sampleRate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16000\u001b[39m\n\u001b[1;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Files\u001b[39m\u001b[38;5;124m\"\u001b[39m, wavenet\u001b[38;5;241m.\u001b[39mrecFields, sampleSize, sampleRate, wavenet\u001b[38;5;241m.\u001b[39mskipChannels)\n\u001b[0;32m----> 6\u001b[0m train(wavenet, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.002\u001b[39m, numSteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, dataL\u001b[38;5;241m=\u001b[39mloader, saveDir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[33], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, lr, numSteps, dataL, saveDir)\u001b[0m\n\u001b[1;32m     27\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m batch(dataL):\n\u001b[0;32m---> 29\u001b[0m     loss \u001b[38;5;241m=\u001b[39m trainModel(model, lr, inputs, targets)\n\u001b[1;32m     30\u001b[0m     steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;241m>\u001b[39m num_steps:\n","Cell \u001b[0;32mIn[33], line 5\u001b[0m, in \u001b[0;36mtrainModel\u001b[0;34m(model, lr, inputs, targets)\u001b[0m\n\u001b[1;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      4\u001b[0m model_loss \u001b[38;5;241m=\u001b[39m loss(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m.\u001b[39mskipChannels), targets\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:33\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(weight_decay))\n\u001b[1;32m     29\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m     30\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m     31\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m     32\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(params, defaults)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:187\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    185\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    189\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n","\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"]}],"source":["wavenet = WaveNetModel(layers=5, layerSize=10, resChannels=512, skipChannels=256)\n","sampleSize = 100000\n","sampleRate = 16000\n","loader = DataLoader(\"./Files\", wavenet.recFields, sampleSize, sampleRate, wavenet.skipChannels)\n","\n","train(wavenet, lr=0.002, numSteps=100000, dataL=loader, saveDir = \"./outputs\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":2}
